{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import preprocessing \n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import pairwise_distances, log_loss\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metricmapper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_scale(X, N=100, inp=\"point cloud\", beta=0., C=10.):\n",
    "    \"\"\"\n",
    "    Compute estimated scale of a point cloud or a distance matrix.\n",
    "\n",
    "    Parameters:\n",
    "        X (numpy array of shape (num_points) x (num_coordinates) if point cloud and (num_points) x (num_points) \n",
    "            if distance matrix): input point cloud or distance matrix.\n",
    "        N (int): subsampling iterations (default 100). \n",
    "        inp (string): either \"point cloud\" or \"distance matrix\". Type of input data (default \"point cloud\").\n",
    "        beta (double): exponent parameter (default 0.).\n",
    "        C (double): constant parameter (default 10.).\n",
    "\n",
    "    Returns:\n",
    "        delta (double): estimated scale that can be used with eg agglomerative clustering.\n",
    "    \"\"\"\n",
    "    num_pts = X.shape[0]\n",
    "    delta, m = 0., int(  num_pts / np.exp((1+beta) * np.log(np.log(num_pts)/np.log(C)))  )\n",
    "    for _ in range(N):\n",
    "        subpop = np.random.choice(num_pts, size=m, replace=False)\n",
    "        if inp == \"point cloud\":\n",
    "            d, _, _ = directed_hausdorff(X, X[subpop,:])\n",
    "        if inp == \"distance matrix\":\n",
    "            d = np.max(np.min(X[:,subpop], axis=1), axis=0)\n",
    "        delta += d/N\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper2networkx(M, get_attrs=False):\n",
    "    \"\"\"\n",
    "    Turn the 1-skeleton of M (computed after calling fit() method) into a networkx graph.\n",
    "    This function requires networkx (https://networkx.org/documentation/stable/install.html).\n",
    "\n",
    "    Parameters:\n",
    "        M (MetricMapperComplex): simplicial complex\n",
    "        get_attrs (bool): if True, the color functions will be used as attributes for the networkx graph.\n",
    "\n",
    "    Returns:\n",
    "        G (networkx graph): graph representing the 1-skeleton of the cover complex.\n",
    "    \"\"\"\n",
    "    st = M.mapper_\n",
    "    G = nx.Graph()\n",
    "    for (splx,_) in st.get_skeleton(1):\n",
    "        if len(splx) == 1:\n",
    "            G.add_node(splx[0])\n",
    "        if len(splx) == 2:\n",
    "            G.add_edge(splx[0], splx[1])\n",
    "    if get_attrs:\n",
    "        attrs = {k: {\"attr_name\": M.node_info[k][\"colors\"]} for k in G.nodes()}\n",
    "        nx.set_node_attributes(G, attrs)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annulus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underlying manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pts = 5000\n",
    "radius  = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta   = np.random.uniform(low=0., high=2*np.pi, size=num_pts)\n",
    "xs, ys  = radius * np.cos(theta), radius * np.sin(theta)\n",
    "noise_x = np.random.normal(loc=xs, scale=.1, size=num_pts)\n",
    "noise_y = np.random.normal(loc=ys, scale=.1, size=num_pts)\n",
    "X       = np.hstack([np.reshape(xs+noise_x, [-1,1]), np.reshape(ys+noise_y, [-1,1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.scatter(X[:,0], X[:,1], s=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = estimate_scale(X, 100)\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_pts):\n",
    "    distributions.append(np.random.normal(loc=X[i,0], scale=0.5, size=num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bimodal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_pts):\n",
    "    distrib = []\n",
    "    d1 = np.random.normal(loc=X[i,0]+2,  scale=0.1, size=num_samples)\n",
    "    d2 = np.random.normal(loc=-X[i,0]-2, scale=0.1, size=num_samples)\n",
    "    distrib = np.concatenate([d1[:int(num_samples/2)], d2[:int(num_samples/2)]])\n",
    "    np.random.shuffle(distrib)\n",
    "    distributions.append(distrib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "plt.hist(np.array(distributions[0]), bins=300, range=[-4.3,4.3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z = [distributions[i][0] for i in range(num_pts)]\n",
    "z = [np.mean(distrib) for distrib in distributions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax  = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(X[:,0], X[:,1], z, s=1.)\n",
    "ax.view_init(elev=33, azim=64)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = []\n",
    "for i in range(num_pts):\n",
    "    real.append(np.random.normal(loc=X[i,0], scale=0.5, size=1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bimodal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = []\n",
    "for i in range(num_pts):\n",
    "    idx = np.random.choice(2, 1)\n",
    "    if idx == 0:\n",
    "        real.append(np.random.normal(loc=X[i,0]+2, scale=0.1, size=1)[0])\n",
    "    else:\n",
    "        real.append(np.random.normal(loc=-X[i,0]-2, scale=0.1, size=1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax  = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(X[:,0], X[:,1], real, s=1.)\n",
    "ax.view_init(elev=33, azim=64)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, m = max(X[:,0]), min(X[:,0])\n",
    "for i in range(num_pts):\n",
    "    A = np.random.binomial(1, (X[i,0]-m)/(M-m), size=(5,5))\n",
    "    Aplus, Aminus = np.triu(A), np.triu(A,1).T\n",
    "    real.append(nx.Graph(Aplus + Aminus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "nx.draw_networkx(real[60], with_labels=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single realization Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover = HypercubeCover(cover_mode=\"implicit\", \n",
    "                       bnds=np.array([[np.array(real).min(), np.array(real).max()]]), \n",
    "                       resolutions=np.array([15]), \n",
    "                       gains=np.array([.3]))\n",
    "\n",
    "mapper = MetricMapperComplex(\n",
    "    filters=np.array(real)[:,np.newaxis], colors=X[:,0:1], codomain=\"vectors\", cover=cover,\n",
    "    clustering=AgglomerativeClustering(n_clusters=None, linkage=\"single\", distance_threshold=1.)\n",
    "                            ).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = mapper2networkx(mapper)\n",
    "plt.figure()\n",
    "nx.draw_networkx(G, with_labels=False,\n",
    "                 node_color=[mapper.node_info_[name][\"colors\"][0] for name in G.nodes()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean-based Mapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = infer_distributions_from_neighborhood(real, X, 3*delta, \"point cloud\")\n",
    "means = np.array([np.mean(distrib) for distrib in distributions])[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cover = HypercubeCover(cover_mode=\"implicit\", \n",
    "                       bnds=np.array([[means.min(), means.max()]]), \n",
    "                       resolutions=np.array([10]), \n",
    "                       gains=np.array([.3]))\n",
    "\n",
    "mapper = MetricMapperComplex(\n",
    "    filters=means, colors=X[:,0:1], cover=cover, codomain=\"vectors\",\n",
    "    clustering=AgglomerativeClustering(n_clusters=None, linkage=\"single\", distance_threshold=delta)\n",
    "                            ).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = mapper2networkx(mapper)\n",
    "plt.figure()\n",
    "nx.draw_networkx(G, with_labels=False,\n",
    "                 node_color=[mapper.node_info_[name][\"colors\"][0] for name in G.nodes()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram-based Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = MetricMapperComplex(\n",
    "    filters=real, codomain=\"distributions\", infer_distributions=True, threshold=1., num_bins=100, \n",
    "    mode=\"NW\", kernel=GaussianKernel(h=0.1),\n",
    "    cover=kPDTMCover(num_patches=10, h=3, threshold=delta/10, tol=1e-7),\n",
    "    correct_Rips=False, delta=delta, num_subdivisions=10,\n",
    "    colors=np.reshape(X[:,0], [-1,1]), mask=5,\n",
    "    clustering=AgglomerativeClustering(n_clusters=None, linkage=\"single\", distance_threshold=delta)\n",
    "                            ).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = mapper2networkx(mapper)\n",
    "plt.figure()\n",
    "nx.draw_networkx(G, with_labels=False,\n",
    "                 node_color=[mapper.node_info_[name][\"colors\"][0] for name in G.nodes()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distances for conditional probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = infer_distributions_from_neighborhood(real, X, 3*delta, \"point cloud\")\n",
    "H, _ = Histogram(num_bins=100).fit_transform(distributions)\n",
    "dists = EuclideanDistance().compute_matrix(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distances for combinatorial graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = np.zeros([num_pts, num_pts])\n",
    "for i in range(num_pts):\n",
    "    for j in range(i+1, num_pts):\n",
    "        GED = nx.optimize_graph_edit_distance(real[i], real[j], upper_bound=20)\n",
    "        for d in GED:\n",
    "            dists[i,j] = d\n",
    "        dists[j,i] = dists[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = MetricMapperComplex(\n",
    "    filters=dists, codomain=\"distance matrix\", \n",
    "    cover=VoronoiCover(num_patches=10, threshold=0.01), distance=EuclideanDistance(),\n",
    "    colors=np.reshape(X[:,0], [-1,1]), mask=5,\n",
    "    clustering=AgglomerativeClustering(n_clusters=None, linkage=\"single\", distance_threshold=delta)\n",
    "                            ).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = mapper2networkx(mapper)\n",
    "plt.figure()\n",
    "nx.draw_networkx(G, with_labels=False,\n",
    "                 node_color=[mapper.node_info_[name][\"colors\"][0] for name in G.nodes()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=5000, n_features=2, random_state=42, cluster_std=5.0)\n",
    "X_train, y_train = X[:3000], y[:3000]\n",
    "X_valid, y_valid = X[3000:4000], y[3000:4000]\n",
    "X_train_valid, y_train_valid = X[:4000], y[:4000]\n",
    "X_test, y_test = X[4000:], y[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], c=y, s=5, cmap=\"rainbow\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train uncalibrated random forest classifier on whole train and validation data and evaluate on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=25)\n",
    "clf.fit(X_train_valid, y_train_valid)\n",
    "clf_probs = clf.predict_proba(X_test)\n",
    "score = log_loss(y_test, clf_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train random forest classifier, calibrate on validation data and evaluate on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=25)\n",
    "clf.fit(X_train, y_train)\n",
    "clf_probs = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover = HypercubeCover(cover_mode=\"implicit\", \n",
    "                       bnds=np.array([[0.,1.],[0.,1.],[0.,1.]]), \n",
    "                       resolutions=np.array([10,10,10]), \n",
    "                       gains=np.array([.3,.3,.3]))\n",
    "\n",
    "mapper = MetricMapperComplex(\n",
    "    filters=clf_probs, colors=clf_probs, codomain=\"vectors\", cover=cover,\n",
    "    #correct_Rips=False, delta=10, correct_mode=\"cover_refinement\",\n",
    "    clustering=AgglomerativeClustering(n_clusters=None, linkage=\"single\", distance_threshold=10)\n",
    "                            ).fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mapper.mapper_.num_vertices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "G = stm.mapper2networkx(mapper)\n",
    "plt.figure()\n",
    "nx.draw_networkx(G, with_labels=False, pos=nx.kamada_kawai_layout(G), \n",
    "                 node_color=[np.var(mapper.node_info_[name][\"colors\"]) for name in G.nodes()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accelero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be downloaded at https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path= \"./uci/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = file_path + \"train/X_train.txt\"\n",
    "activity_features = pd.read_csv(data_path, delim_whitespace=True, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = file_path + \"train/y_train.txt\"\n",
    "activity  = pd.read_csv(data_path, delim_whitespace=True, header=None)\n",
    "activity  = activity.values[:,0] - 1\n",
    "activity_names = ['WALKING','WALKING_UPSTAIRS','WALKING_DOWNSTAIRS','SITTING','STANDING','LAYING']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = file_path + \"train/subject_train.txt\"\n",
    "sujet = pd.read_csv(data_path, delim_whitespace=True, header=None)\n",
    "sujet = sujet.values[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nor = preprocessing.scale(activity_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use naive Bayes for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(features_nor, activity)\n",
    "pred = gnb.predict(features_nor)\n",
    "scores = model_selection.cross_val_score(GaussianNB(), features_nor, activity, cv=10)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimation of a posteriori probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = gnb.predict_proba(features_nor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = estimate_scale(features_nor, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover = HypercubeCover(cover_mode=\"implicit\", \n",
    "                       bnds=np.array([[0.,1.],[0.,1.],[0.,1.],[0.,1.],[0.,1.],[0.,1.]]), \n",
    "                       resolutions=np.array([6,6,6,6,6,6]), \n",
    "                       gains=np.array([.4,.4,.4,.4,.4,.4]))\n",
    "\n",
    "mapper = MetricMapperComplex(\n",
    "    filters=posterior, colors=posterior, codomain=\"vectors\", cover=cover,\n",
    "    clustering=AgglomerativeClustering(n_clusters=None, linkage=\"single\", distance_threshold=delta)\n",
    "                            ).fit(features_nor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "G = stm.mapper2networkx(mapper)\n",
    "plt.figure()\n",
    "nx.draw_networkx(G, with_labels=False,\n",
    "                 node_color=[np.var(mapper.node_info_[name][\"colors\"]) for name in G.nodes()],\n",
    "                 vmin=0., vmax=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
